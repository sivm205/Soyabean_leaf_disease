{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sivm205/plant-leaf-disease-detection-and-classification/blob/master/soya_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3F9ifQS8andq"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "utoykUYy7YOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jkuvNmRAawil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lT_B10PJandt"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = 224\n",
        "CHANNELS=3\n",
        "EPOCHS=50"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# name of the zip file to extract\n",
        "zip_file_name = \"example.zip\"\n",
        "\n",
        "# open the zip file for reading\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/soybean.leaf.dataset.zip\", 'r') as zip_ref:\n",
        "    # extract all files to a specified directory\n",
        "    zip_ref.extractall(\"/content/plant/\")\n"
      ],
      "metadata": {
        "id": "Ip2_vEQgbFUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfOhL1gwandt"
      },
      "outputs": [],
      "source": [
        "path = \"/content/plant\"\n",
        "dataset = tf.keras.preprocessing.image_dataset_from_directory(path,\n",
        "    seed=123,\n",
        "    shuffle=True,\n",
        "    image_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJJNWY14andu"
      },
      "outputs": [],
      "source": [
        "classes= dataset.class_names\n",
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aU5qAcEJandu"
      },
      "outputs": [],
      "source": [
        "for image_batch, labels_batch in dataset.take(1): #only for one batch\n",
        "    print(image_batch.shape)\n",
        "    print(labels_batch.numpy()) #representing single batch images in the form of array\n",
        "    #each value represent a single image from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOTgt4Eeandu"
      },
      "outputs": [],
      "source": [
        "labels_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8NLtvuhandv"
      },
      "source": [
        "### visualize some of the sample images from the datset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QaP8m_Tandw"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "for image_batch, labels_batch in dataset.take(1):\n",
        "    for i in range(12):\n",
        "        ax = plt.subplot(3, 4, i + 1)\n",
        "        plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(class_names[labels_batch[i]])\n",
        "        plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7H8lwgkandx"
      },
      "outputs": [],
      "source": [
        "len(dataset) #number of total btach file, in single batch there will be 32 image sample "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCRv38w8andx"
      },
      "outputs": [],
      "source": [
        "train_size = 0.8\n",
        "len(dataset)*train_size #80 percent of the data i will use for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wrnkPTHandy"
      },
      "outputs": [],
      "source": [
        "train_ds = dataset.take(524) #total 524 out of 655 batch set used as a training set \n",
        "len(train_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fObNXxLnandy"
      },
      "outputs": [],
      "source": [
        "test_ds = dataset.skip(524) #and the rest will used as validation and testing set\n",
        "#it will start counting after the given number\n",
        "len(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3SzOTO-andz"
      },
      "outputs": [],
      "source": [
        "val_size=0.1\n",
        "len(dataset)*val_size  #10 percent will used as validation set from test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBYl06ZSandz"
      },
      "outputs": [],
      "source": [
        "val_ds = test_ds.take(65)\n",
        "len(val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9rX_xa_and0"
      },
      "outputs": [],
      "source": [
        "test_ds = test_ds.skip(65)\n",
        "len(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6vf978zand0"
      },
      "outputs": [],
      "source": [
        "def get_dataset_partitions_tf(ds, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
        "    assert (train_split + test_split + val_split) == 1\n",
        "    \n",
        "    ds_size = len(ds)\n",
        "    \n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(shuffle_size, seed=12)\n",
        "    \n",
        "    train_size = int(train_split * ds_size)\n",
        "    val_size = int(val_split * ds_size)\n",
        "    \n",
        "    train_ds = ds.take(train_size)    \n",
        "    val_ds = ds.skip(train_size).take(val_size)\n",
        "    test_ds = ds.skip(train_size).skip(val_size)\n",
        "    \n",
        "    return train_ds, val_ds, test_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBcITfn_and0"
      },
      "outputs": [],
      "source": [
        "train_ds, val_ds, test_ds = get_dataset_partitions_tf(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSf7EEE0and1"
      },
      "outputs": [],
      "source": [
        "len(train_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSRsSdlMand1"
      },
      "outputs": [],
      "source": [
        "len(val_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DN2P_I1eand1"
      },
      "outputs": [],
      "source": [
        "len(test_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_S5KPf8Nand1"
      },
      "outputs": [],
      "source": [
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "'''\n",
        "overall this method are used to optimize the processing of each batch dataset\n",
        "with the goal of reducing the overall processing time'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEpo3ogNand1"
      },
      "outputs": [],
      "source": [
        "resize_and_rescale = tf.keras.Sequential([\n",
        "  layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
        "  layers.experimental.preprocessing.Rescaling(1./255),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBhejmvtand2"
      },
      "outputs": [],
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LokIQOyjand2"
      },
      "outputs": [],
      "source": [
        "train_ds = train_ds.map(\n",
        "    lambda x, y: (data_augmentation(x, training=True), y)\n",
        ").prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CezeeYy-and2"
      },
      "outputs": [],
      "source": [
        "#base model\n",
        "input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
        "n_classes = 9\n",
        "\n",
        "model = models.Sequential([\n",
        "    resize_and_rescale,\n",
        "    layers.Conv2D(32, kernel_size = (3,3), activation='relu', input_shape=input_shape),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(n_classes, activation='softmax'),\n",
        "])\n",
        "\n",
        "model.build(input_shape=input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tuned hyperparameter\n",
        "\n",
        "from tensorflow.keras import models, layers, optimizers, callbacks\n",
        "\n",
        "# Define hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = 224\n",
        "CHANNELS = 3\n",
        "n_classes = 9\n",
        "epochs = 50\n",
        "initial_learning_rate = 0.001\n",
        "dropout_rate = 0.5\n",
        "weight_decay = 0.0001\n",
        "patience = 5\n",
        "factor = 0.5\n",
        "\n",
        "# Define preprocessing layers\n",
        "resize_and_rescale = tf.keras.Sequential([\n",
        "    layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    layers.experimental.preprocessing.Rescaling(1./255)\n",
        "])\n",
        "\n",
        "# Define the model architecture with regularization\n",
        "model = models.Sequential([\n",
        "    resize_and_rescale,\n",
        "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(weight_decay), input_shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)),\n",
        "    layers.Dropout(dropout_rate),\n",
        "    layers.Dense(n_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model with learning rate scheduling\n",
        "opt = optimizers.Adam(learning_rate=initial_learning_rate)\n",
        "lr_schedule = callbacks.ReduceLROnPlateau(factor=factor, patience=patience)\n",
        "model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = callbacks.EarlyStopping(patience=patience, restore_best_weights=True)\n",
        "\n",
        "# Train the model with callbacks\n",
        "history = model.fit(train_ds,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    validation_data=val_ds,\n",
        "                    epochs=epochs,\n",
        "                    callbacks=[early_stopping, lr_schedule],\n",
        "                    verbose=1)\n"
      ],
      "metadata": {
        "id": "AJPTxYBojXO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#using transfer learning\n",
        "from tensorflow.keras import models, layers, optimizers, callbacks\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# Define hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = 224\n",
        "CHANNELS = 3\n",
        "n_classes = 9\n",
        "epochs = 50\n",
        "initial_learning_rate = 0.001\n",
        "dropout_rate = 0.5\n",
        "weight_decay = 0.0001\n",
        "patience = 5\n",
        "factor = 0.5\n",
        "\n",
        "# Define preprocessing layers\n",
        "resize_and_rescale = tf.keras.Sequential([\n",
        "    layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    layers.experimental.preprocessing.Rescaling(1./255)\n",
        "])\n",
        "\n",
        "# Load the pre-trained model\n",
        "base_model = hub.KerasLayer(\"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/5\",\n",
        "                            trainable=False)\n",
        "\n",
        "# Add new classification layers on top\n",
        "model = models.Sequential([\n",
        "    resize_and_rescale,\n",
        "    base_model,\n",
        "    layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(weight_decay)),\n",
        "    layers.Dropout(dropout_rate),\n",
        "    layers.Dense(n_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model with learning rate scheduling\n",
        "opt = optimizers.Adam(learning_rate=initial_learning_rate)\n",
        "lr_schedule = callbacks.ReduceLROnPlateau(factor=factor, patience=patience)\n",
        "model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = callbacks.EarlyStopping(patience=patience, restore_best_weights=True)\n",
        "\n",
        "# Train the model with callbacks\n",
        "history = model.fit(train_ds,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    validation_data=val_ds,\n",
        "                    epochs=epochs,\n",
        "                    callbacks=[early_stopping, lr_schedule],\n",
        "                    verbose=1)\n"
      ],
      "metadata": {
        "id": "MpEIk_SnfySV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# Save the history object using pickle\n",
        "with open('history.pkl', 'wb') as file:\n",
        "    pickle.dump(history.history, file)\n"
      ],
      "metadata": {
        "id": "MxmW9mmvmO1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWFMZQqLand2"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qm-0UOt_and3"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2QZOEfoand3"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=val_ds,\n",
        "    verbose=1,\n",
        "    epochs=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmYZLOGYand3"
      },
      "outputs": [],
      "source": [
        "scores = model.evaluate(test_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing on a test set\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "np.random.seed(12)\n",
        "random.seed(12)\n",
        "tf.random.set_seed(12)\n",
        "\n",
        "# Load the trained model\n",
        "#model = load_model('D:/Programs/python/Data_Science/Leaf detection/research implementation/Soyabean_Model.h5')\n",
        "\n",
        "# Evaluate the model\n",
        "score = model.evaluate_generator(test_ds, steps=len(test_ds))\n",
        "print(\"Test accuracy: {:.2f}%\".format(score[1]*100))\n",
        "print(\"Test loss: {:.4f}\".format(score[0]))\n",
        "\n"
      ],
      "metadata": {
        "id": "KDgAwFOCGw8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ly-IdmvCand3"
      },
      "outputs": [],
      "source": [
        "scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7q6VvUiland4"
      },
      "outputs": [],
      "source": [
        "history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJUuHp9Kand4"
      },
      "outputs": [],
      "source": [
        "history.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXvF2r1Qand4"
      },
      "outputs": [],
      "source": [
        "history.history.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQhiUxPqand4"
      },
      "outputs": [],
      "source": [
        "type(history.history['loss'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYZ2Aoviand4"
      },
      "outputs": [],
      "source": [
        "len(history.history['loss'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COygzo6zand5"
      },
      "outputs": [],
      "source": [
        "history.history['loss'][:5] # show loss for first 5 epochs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RBbSZqWand5"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQ2Jt2Leand5"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(28), acc, label='Training Accuracy')\n",
        "plt.plot(range(28), val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(28), loss, label='Training Loss')\n",
        "plt.plot(range(28), val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omrDXxq4and5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "for images_batch, labels_batch in test_ds.take(1):\n",
        "    \n",
        "    first_image = images_batch[0].numpy().astype('uint8')\n",
        "    first_label = labels_batch[0].numpy()\n",
        "    \n",
        "    print(\"first image to predict\")\n",
        "    plt.imshow(first_image)\n",
        "    print(\"actual label:\",class_names[first_label])\n",
        "    \n",
        "    batch_prediction = model.predict(images_batch)\n",
        "    print(\"predicted label:\",class_names[np.argmax(batch_prediction[0])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQ8q1HPUand5"
      },
      "outputs": [],
      "source": [
        "def predict(model, img):\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())\n",
        "    img_array = tf.expand_dims(img_array, 0)\n",
        "\n",
        "    predictions = model.predict(img_array)\n",
        "\n",
        "    predicted_class = class_names[np.argmax(predictions[0])]\n",
        "    confidence = round(100 * (np.max(predictions[0])), 2)\n",
        "    return predicted_class, confidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-64PMZtand6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 15))\n",
        "for images, labels in test_ds.take(1):\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        \n",
        "        predicted_class, confidence = predict(model, images[i].numpy())\n",
        "        actual_class = class_names[labels[i]] \n",
        "        \n",
        "        plt.title(f\"Actual: {actual_class},\\n Predicted: {predicted_class}.\\n Confidence: {confidence}%\")\n",
        "        \n",
        "        plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQeBUzJ8and6"
      },
      "outputs": [],
      "source": [
        "model.save(\"Soyabean_Model3.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n"
      ],
      "metadata": {
        "id": "7hOe4E7DJVFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9af7qM1tand6"
      },
      "outputs": [],
      "source": [
        "y_true = list(test_ds.map(lambda x, y: y).unbatch().as_numpy_iterator())\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(test_ds)\n",
        "\n",
        "# Convert predicted probabilities to predicted class labels\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Calculate classification report and confusion matrix\n",
        "print(classification_report(y_true, y_pred))\n",
        "print(confusion_matrix(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8isdLSqand6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Define the predicted and true labels for the test set\n",
        "y_pred = model.predict(test_ds)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
        "\n",
        "# Create the confusion matrix\n",
        "conf_mat = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot the confusion matrix using Seaborn library\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_mat, annot=True, cmap='Blues', fmt='g', xticklabels=classes, yticklabels=classes)\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAlwMWfCand6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrKhyaEnand6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "c25269a4018224bb4e3cb6b79397037e31419b0dedc1b97e47175df2e08dbf7b"
      }
    },
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}